{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Result Management System**"
      ],
      "metadata": {
        "id": "GHt7gBxeyy9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Result Management System is designed to efficiently process, analyze, and manage student academic records using Apache Spark, MongoDB, and Kafka. This system handles large datasets, performs statistical analysis, and provides insights into student performance.\n",
        "\n",
        "### Key Features:\n",
        "### 1: Data Generation & Storage – Generates student records and stores them in MongoDB.\n",
        "### 2: Data Processing – Uses PySpark to clean, filter, and analyze student results.\n",
        "### 3: Statistical Analysis – Computes performance metrics like average, highest, and lowest scores.\n",
        "### 4: Visualization – Displays trends in student marks using histograms and summary statistics.\n",
        "### 5:Real-time Streaming (Optional) – Uses Kafka for real-time updates and notifications.\n",
        "\n",
        "### This project showcases the power of Big Data technologies in managing academic records efficiently."
      ],
      "metadata": {
        "id": "BYMpNWmEzPJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Libraries"
      ],
      "metadata": {
        "id": "yyZEha6cygdA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIo26jivnTIj",
        "outputId": "0e7be680-4470-4ea3-8bec-db48f6fb8e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.11/dist-packages (2.0.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.11.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark faker kafka-python pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing prerequisites"
      ],
      "metadata": {
        "id": "GJanDtA1yR1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "from faker import Faker\n",
        "import random\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from pymongo import MongoClient\n",
        "from pyspark.sql.functions import avg, max, min, col\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "CeLkeos8nhWC"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialising Spark Session\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BqowD5YoyLr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"ResultManagement\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(spark)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YelDPsKNoJNC",
        "outputId": "481d05eb-5ae4-46d6-902e-bad667cde501"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7d9c0f313c10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Data using Faker"
      ],
      "metadata": {
        "id": "pDOIOvpKyHc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "def generate_students(num_students=10000):\n",
        "    student_ids = list(range(1000, 1000 + num_students))\n",
        "    random.shuffle(student_ids)\n",
        "\n",
        "    students = [(student_ids[i], fake.name(), random.randint(35, 100)) for i in range(num_students)]\n",
        "    return students\n",
        "\n",
        "data = generate_students()\n",
        "columns = [\"ID\", \"Name\", \"Marks\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT2Bv0uhoNJi",
        "outputId": "079ea067-8da1-4d37-8712-f9b2a447a5a1"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+-----+\n",
            "|   ID|             Name|Marks|\n",
            "+-----+-----------------+-----+\n",
            "| 4431|   Jerry Hamilton|   42|\n",
            "| 7168|   Melanie Spears|   51|\n",
            "| 7684|      James Young|   96|\n",
            "| 9092|   Garrett Butler|   61|\n",
            "| 9961|   Kimberly Yates|   94|\n",
            "| 4460|        Kim Davis|   41|\n",
            "| 1469|Jennifer Martinez|   56|\n",
            "| 4250|      Mark Fisher|   86|\n",
            "|10307|    Tony Richards|   35|\n",
            "| 8510|  Monica Gonzalez|   37|\n",
            "+-----+-----------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting to Spark Dataframe"
      ],
      "metadata": {
        "id": "6mgc7wfRx_xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(students_data)"
      ],
      "metadata": {
        "id": "lxQlhp-roeFV"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Stats(Average)"
      ],
      "metadata": {
        "id": "Fxj6jZgTx8eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stats_df = df.select(\n",
        "    F.lit(\"Electronics\").alias(\"Subject\"),\n",
        "    avg(col(\"Electronics\")).alias(\"Avg_Marks\"),\n",
        "    max(col(\"Electronics\")).alias(\"Max_Marks\"),\n",
        "    min(col(\"Electronics\")).alias(\"Min_Marks\")\n",
        ").union(df.select(\n",
        "    F.lit(\"Programming\").alias(\"Subject\"),\n",
        "    avg(col(\"Programming\")).alias(\"Avg_Marks\"),\n",
        "    max(col(\"Programming\")).alias(\"Max_Marks\"),\n",
        "    min(col(\"Programming\")).alias(\"Min_Marks\")\n",
        ")).union(df.select(\n",
        "    F.lit(\"Database\").alias(\"Subject\"),\n",
        "    avg(col(\"Database\")).alias(\"Avg_Marks\"),\n",
        "    max(col(\"Database\")).alias(\"Max_Marks\"),\n",
        "    min(col(\"Database\")).alias(\"Min_Marks\")\n",
        ")).union(df.select(\n",
        "    F.lit(\"Data Science\").alias(\"Subject\"),\n",
        "    avg(col(\"Data Science\")).alias(\"Avg_Marks\"),\n",
        "    max(col(\"Data Science\")).alias(\"Max_Marks\"),\n",
        "    min(col(\"Data Science\")).alias(\"Min_Marks\")\n",
        ")).union(df.select(\n",
        "    F.lit(\"Mathematics\").alias(\"Subject\"),\n",
        "    avg(col(\"Mathematics\")).alias(\"Avg_Marks\"),\n",
        "    max(col(\"Mathematics\")).alias(\"Max_Marks\"),\n",
        "    min(col(\"Mathematics\")).alias(\"Min_Marks\")\n",
        ")).union(df.select(\n",
        "    F.lit(\"DSA\").alias(\"Subject\"),\n",
        "    avg(col(\"DSA\")).alias(\"Avg_Marks\"),\n",
        "    max(col(\"DSA\")).alias(\"Max_Marks\"),\n",
        "    min(col(\"DSA\")).alias(\"Min_Marks\")\n",
        "))\n",
        "\n",
        "# Show the formatted table\n",
        "stats_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9x1e6hmohdt",
        "outputId": "54de29d9-ea18-44a8-96ee-315432173a7b"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------+---------+---------+\n",
            "|     Subject|Avg_Marks|Max_Marks|Min_Marks|\n",
            "+------------+---------+---------+---------+\n",
            "| Electronics|  69.8027|      100|       40|\n",
            "| Programming|  70.3255|      100|       40|\n",
            "|    Database|  69.6903|      100|       40|\n",
            "|Data Science|  69.9387|      100|       40|\n",
            "| Mathematics|  70.1751|      100|       40|\n",
            "|         DSA|  70.1088|      100|       40|\n",
            "+------------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collecting stats"
      ],
      "metadata": {
        "id": "n5L5WZgTx5-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats = stats_df.collect()[0].asDict()"
      ],
      "metadata": {
        "id": "vAkZHZb-okwG"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialising Kafka producer to send stats"
      ],
      "metadata": {
        "id": "L2bcpuUuxzoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def send_to_kafka(topic, data):\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers='localhost:9092',\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "    )\n",
        "    producer.send(topic, data)\n",
        "    producer.flush()"
      ],
      "metadata": {
        "id": "wPq_rhlGonU8"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sending Computed stats to Kafka topic"
      ],
      "metadata": {
        "id": "ocG7UczGxtJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#send_to_kafka(\"student_statistics\", stats)"
      ],
      "metadata": {
        "id": "ZLsa1Zb3oqN0"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to MongoDB"
      ],
      "metadata": {
        "id": "aC6_1-UqxnRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client[\"ResultManagement\"]\n",
        "collection = db[\"Feedback\"]"
      ],
      "metadata": {
        "id": "vQxJVdCmotcE"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing Feedback in MongoDB"
      ],
      "metadata": {
        "id": "wx0j7FK-xeEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_feedback(student_id, feedback):\n",
        "    collection.insert_one({\"StudentID\": student_id, \"Feedback\": feedback})"
      ],
      "metadata": {
        "id": "Tox2gfZEoypr"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Showing record and computed stats"
      ],
      "metadata": {
        "id": "msFd_dqTxU1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)\n",
        "stats_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP8ZbgfBp_kb",
        "outputId": "fef6fccf-261e-455f-e8bb-b4453d11ba77"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+--------+-----------+-----------+----------------+-----------+---------+\n",
            "|DSA|Data Science|Database|Electronics|Mathematics|            Name|Programming|StudentID|\n",
            "+---+------------+--------+-----------+-----------+----------------+-----------+---------+\n",
            "| 52|          87|      77|         44|         96| Kimberly Garcia|         41|     5820|\n",
            "| 47|          60|      72|         72|         79|      Ryan Smith|         77|     7583|\n",
            "| 51|          72|      52|         75|         81|  Barbara Glover|         97|     4630|\n",
            "| 82|          89|      48|         88|         68|Tamara Hernandez|         43|     6777|\n",
            "| 66|          76|      58|         79|         46|   Carrie Martin|         89|     4018|\n",
            "| 84|          56|      55|         74|         63|    Amanda Klein|         49|     1485|\n",
            "| 65|          88|      86|         90|         69| Christina White|         93|     9688|\n",
            "| 68|          57|      82|         64|         80|      Mark Smith|         97|     9149|\n",
            "| 44|          73|      73|         47|         79|  Daniel Mcclain|         42|     5885|\n",
            "| 91|          85|      60|         60|         95|   Nicole Dennis|         93|     2647|\n",
            "+---+------------+--------+-----------+-----------+----------------+-----------+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------------+---------+---------+---------+\n",
            "|     Subject|Avg_Marks|Max_Marks|Min_Marks|\n",
            "+------------+---------+---------+---------+\n",
            "| Electronics|  69.8027|      100|       40|\n",
            "| Programming|  70.3255|      100|       40|\n",
            "|    Database|  69.6903|      100|       40|\n",
            "|Data Science|  69.9387|      100|       40|\n",
            "| Mathematics|  70.1751|      100|       40|\n",
            "|         DSA|  70.1088|      100|       40|\n",
            "+------------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.csv(\"students_data.csv\", header=True, mode=\"overwrite\")\n",
        "stats_df.write.csv(\"statistics.csv\", header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "eA9-jR6FxGI5"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Terminating Spark Session"
      ],
      "metadata": {
        "id": "zI31JvFbz6zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "j8_mskTYqN00"
      },
      "execution_count": 147,
      "outputs": []
    }
  ]
}
